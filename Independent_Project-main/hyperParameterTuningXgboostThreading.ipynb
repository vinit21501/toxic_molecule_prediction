{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# df=pd.read_excel(\"allfeatures.xlsx\")\n",
    "df=pd.read_csv(\"allfeatures.csv\")\n",
    "print(f'shapte of feateures: {df.shape}')\n",
    "print(f'total na values: {df.isna().sum().sum()}')\n",
    "print(f'total null values: {df.isnull().sum().sum()}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapte of feateures: (7722, 1389)\ntotal na values: 0\ntotal null values: 0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def tune_hyperparameters(X, y):\n",
    "    # Define the XGBoost classifier model\n",
    "    model = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42,\n",
    "        n_jobs=-1  # Utilize all available CPUs for xgboost\n",
    "    )\n",
    "\n",
    "    # Define the parameter grid to search\n",
    "    params = {\n",
    "        'max_depth': [3, 5, 7, 9],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'n_estimators': [100, 200, 300, 400],\n",
    "        'colsample_bytree': [0.5, 0.7, 0.9, 1],\n",
    "        'subsample': [0.5, 0.7, 0.9, 1],\n",
    "        'gamma': [0, 0.1, 0.2, 0.3]\n",
    "    }\n",
    "\n",
    "    # Setup GridSearchCV to find the best hyperparameters\n",
    "    # n_jobs=-1 uses all available CPUs for the grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=params,\n",
    "        scoring='roc_auc',\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        n_jobs=-1  # Utilize all available CPUs for GridSearchCV\n",
    "    )\n",
    "\n",
    "    # Fit GridSearchCV on the provided training data\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Output the best parameters and the best AUC score achieved\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    print(\"Best AUC: {:.4f}\".format(grid_search.best_score_))\n",
    "\n",
    "    # Return the model with the best parameters\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Feature matrix and label vector as specified\n",
    "X = df.drop(['pcid', 'class', 'smile'], axis=1)\n",
    "y = df['class']\n",
    "\n",
    "# Call the function with your dataset\n",
    "best_model = tune_hyperparameters(X, y)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming 'df' is your DataFrame with the necessary columns\n",
    "X = df.drop(['class', 'pcid', 'smile'], axis=1)  # feature matrix\n",
    "y = df['class']  # label vector (binary)\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling for better performance of XGBoost not necessary but can be performed if part of your preprocessing\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_proba = model.predict_proba(X_test_scaled)[:, 1]  # probabilities for positive class\n",
    "\n",
    "# Calculating the evaluation metrics\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Printing the metrics\n",
    "print(f\"ROC AUC Score: {auc:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score: 0.94\nF1 Score: 0.86\nPrecision: 0.86\nRecall: 0.86\nAccuracy: 0.87\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming 'df' is your DataFrame and it's already loaded\n",
    "X = df.drop(['class', 'smile', 'pcid'], axis=1)  # feature matrix\n",
    "y = df['class']  # label vector (binary)\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling for XGBoost which is not necessary but included for completeness\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the model\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300]\n",
    "}\n",
    "\n",
    "# Setup the grid search\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "# Retrieve the best model from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predicting the test set results\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "y_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculating the evaluation metrics\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Printing the metrics\n",
    "print(f\"ROC AUC Score: {auc:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
      "Best parameters: {'learning_rate': 0.1, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 200}\nBest cross-validation score: 0.87\nROC AUC Score: 0.95\nF1 Score: 0.87\nPrecision: 0.87\nRecall: 0.87\nAccuracy: 0.88\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n",
    "\n",
    "\n",
    "params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [4, 5, 6, 7, 8]\n",
    "        }\n",
    "\n",
    "xgb = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic', nthread=2)\n",
    "\n",
    "folds = 10\n",
    "param_comb = 5\n",
    "\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "\n",
    "random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=10, cv=skf.split(X_train_scaled, y_train), verbose=3, random_state=1001 )\n",
    "\n",
    "# Here we go\n",
    "start_time = timer(None) # timing starts from this point for \"start_time\" variable\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "timer(start_time) # timing ends here for \"start_time\" variable"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
      "\n Time taken: 0 hours 2 minutes and 49.78 seconds.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "source": [
    "#print('\\n All results:')\n",
    "#print(random_search.cv_results_)\n",
    "print('\\n Best estimator:')\n",
    "print(random_search.best_estimator_)\n",
    "print('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n",
    "print(random_search.best_score_ * 2 - 1)\n",
    "print('\\n Best hyperparameters:')\n",
    "print(random_search.best_params_)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n Best estimator:\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=1.0, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=1, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.02, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=7, max_leaves=None,\n              min_child_weight=5, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=600, n_jobs=None, nthread=2,\n              num_parallel_tree=None, ...)\n\n Best normalized gini score for 10-fold search with 5 parameter combinations:\n0.8929648651599802\n\n Best hyperparameters:\n{'subsample': 0.8, 'min_child_weight': 5, 'max_depth': 7, 'gamma': 1, 'colsample_bytree': 1.0}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "source": [
    "y_pred = random_search.predict(X_test_scaled)\n",
    "y_proba = random_search.predict_proba(X_test_scaled)[:, 1]\n",
    " \n",
    "# Calculating the evaluation metrics\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    " \n",
    "# Printing the metrics \n",
    "print(f\"ROC AUC Score: {auc:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score: 0.95\nF1 Score: 0.88\nPrecision: 0.88\nRecall: 0.87\nAccuracy: 0.88\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
